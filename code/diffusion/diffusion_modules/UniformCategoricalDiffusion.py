from tqdm.auto import tqdm
import logging

import torch
import torch.nn.functional as f

from ..diffusion_modules.BaseDiffusion import Diffusion
from ..utils.stuff import unsqueeze_n, cum_matmul, cat_dist


class UniformCategoricalDiffusion(Diffusion):
    def __init__(self, n_categorical_vars, n_values, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.n_categorical_vars = n_categorical_vars  # Number of categorical variables
        self.values = n_values  # Number of values for each categorical variable

    def sample_from_noise_fn(self, ts, bar=False):
        if bar:
            samples = [self.get_qt_bar(t) for t in ts]
        else:
            samples = [self.get_qt(t) for t in ts]
        return torch.stack(samples).to(self.device)

    def get_qt(self, t):
        """
        Defined by Austin et al. 2023, appendix 2.1. We use the linear algebra notation.
        Obtains the matrix Q_t.
        """
        if isinstance(t, torch.Tensor):
            t = t.clone()
        t -= 1
        k = self.values
        q1 = unsqueeze_n(1 - self.beta[t], 2) * torch.eye(k).unsqueeze(0)
        q2 = unsqueeze_n(self.beta[t], 2) * torch.ones([k, k]).unsqueeze(0)
        q = q1 + q2 / k
        q = q.unsqueeze(1).repeat(1, self.n_categorical_vars, 1, 1)
        return q

    def get_qt_bar(self, t):
        """Obtains the \\bar{Q}_t matrix."""
        if isinstance(t, int):
            t = unsqueeze_n(torch.tensor(t), 2)

        if t.ndim in [0, 1]:
            t = unsqueeze_n(t, 2 - t.ndim)

        qt_bars = []
        for individual_t in t:
            sample = [self.get_qt(i) for i in range(1, individual_t + 1)]
            sample = torch.cat(sample, dim=0)
            sample = cum_matmul(sample, dim=0)
            qt_bars.append(sample)
        return torch.stack(qt_bars)

    def q_xt_given_x0(self, x0, t):
        qt_bar = self.get_qt_bar(t)
        p = torch.einsum("abi,abij->abj", x0, qt_bar)
        return p

    def q_xtsub1_given_xt_x0(self, xt, x0, t):
        """
        Algorithm used from Austin et al. 2023, section 3.
        Computes p_theta(x_{t-1} | x_t) which is approximated by
        the sum of q(x_{t-1} | x_t, x_0)p_\theta(x_0 | x_t) over all
        probabilities generated by the model.
        """
        qt = self.get_qt(t)  # qt(t)
        qt_transpose = qt.transpose(-1, -2)
        qt_sub_bar = self.get_qt_bar(t - 1)  # qt_bar(t-1)
        qt_bar = self.get_qt_bar(t)  # qt_bar(t)
        q1 = torch.einsum("abi,abij->abj", xt, qt_transpose)
        q2 = torch.einsum("abi,abij->abj", x0, qt_sub_bar)
        q3 = torch.einsum("abi,abij->abj", x0, qt_bar)
        q4 = torch.einsum("abi,abi->ab", q3, xt)
        q5 = q1 * q2
        q = q5 / q4.unsqueeze(-1)
        return q

    def q_xtsub1_x1_given_x0(self, xt, x0, t):
        xtsub1_given_xt_x0 = self.q_xtsub1_given_xt_x0(xt, x0, t)
        x1_given_x0 = self.q_xt_given_x0(x0, t)
        xtsub1_x1_given_x0 = torch.einsum("abi,abj->abij", xtsub1_given_xt_x0, x1_given_x0)
        return xtsub1_x1_given_x0

    def p_previous_x(self, xt, model_out, t):
        """
        Computes p_theta(x_{t-1} | x_t) according to Austin et al. 2023, section 3.
        Also handles cases where t == 1, where it returns the model output p_theta(x_0 | x_1)
        """
        # Computes the model out, i.e. p_theta(x_0 | x_t)

        # If t == 1 then p_theta(x_{t-1} | x_t) = p_theta(x_0 | x_1)
        # Which is the model output.
        if t == 1:
            return model_out
        # Here we create a tensor of all different states, i.e. one hot encodings,
        # with the proper shape
        all_eyes = torch.eye(model_out.size(-1)).unsqueeze(1).unsqueeze(1)
        all_eyes = all_eyes.repeat(1, model_out.size(0), model_out.size(1), 1)

        # For each one hot vector representing x0, we compute q(x_{t-1},x_t | x_0)
        p_tmp = torch.stack(
            [self.q_xtsub1_x1_given_x0(xt, tmp_x0, t) for tmp_x0 in all_eyes]
        )

        # Since we already know x_t, we can index in and select the correct probability
        # via a batched dot product.
        p_tmp_2 = torch.einsum("tncij,ncj->tnci", p_tmp, xt)

        # We then map out all p_\theta(x_0 | x_t) and reshape properly
        model_ps = torch.stack(
            [model_out[:, :, i] for i in range(model_out.size(-1))]
        ).unsqueeze(-1).repeat(1, 1, 1, model_out.size(-1))

        # Finally, we compute the sum over all x_0 and normalize
        p_final = (p_tmp_2 * model_ps).sum(dim=0)
        p_final = p_final / p_final.sum(dim=-1, keepdim=True)
        return p_final

    def sample_previous_x(self, xt, t, n, model, labels=None):
        """
        Samples x_{t-1} given x_t, according to Austin et al. 2023, section 3.
        """
        model_out = model(xt, t, labels=labels)
        model_out = f.softmax(model_out, dim=-1)
        p = self.p_previous_x(xt, model_out, t)
        o = cat_dist(p, self.values).float()
        return o

    def diffuse(self, x_0, t):
        """Computes the diffusion process. Takes x_0 and t, and returns x_t and epsilon_t = Q_t."""
        q_t_bar = self.get_qt_bar(t)
        # probs = x_0 @ q_t_bar
        probs = torch.einsum("abi,abij->abj", x_0, q_t_bar)
        o = cat_dist(probs, self.values).float()
        return o, q_t_bar

    def uniform_x(self, n):
        x = torch.randint(0, self.values, (n, self.n_categorical_vars))
        x = f.one_hot(x, self.values).float()
        return x

    def sample(self, model, n, labels=None):
        """Sample n examples from the model, with optional labels for conditional sampling.
        The `labels` argument is ignored if the model is unconditional.
        """
        if self.conditional:
            sample_args = [n, model, labels]
        else:
            sample_args = [n, model]

        logging.info(f"Sampling {n} new categorical features....")
        model.eval()
        with torch.no_grad():
            # x begins as a one hot sample from a uniform distribution.
            x = self.uniform_x(n)
            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):
                x = self.sample_previous_x(x, i, *sample_args)

        model.train()
        return x

    def loss(self, prediction, _noise, batch):
        x0 = batch["x0"]
        return f.cross_entropy(prediction, x0.argmax(dim=-1))
